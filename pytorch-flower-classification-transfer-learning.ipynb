{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22620,
     "status": "ok",
     "timestamp": 1622571688986,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "WxbH_yJKzpMB",
    "outputId": "2869249b-2fa4-4b38-862a-8d38055719a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier/flowers\n",
      "bash: get_datasets.sh: No such file or directory\n",
      "/home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "FOLDERNAME = 'home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier/'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier')\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd /$FOLDERNAME/flowers/\n",
    "!bash get_datasets.sh\n",
    "%cd /$FOLDERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3048,
     "status": "ok",
     "timestamp": 1622571736570,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "j131GT4znaRk",
    "outputId": "4f8ec348-08da-4d76-d7a8-62a7491b18a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rose', 'daisy', 'dandelion', 'flowers', 'sunflower', 'tulip']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "path = '/home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier'\n",
    "print(os.listdir(f'{path}/flowers'))\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "id": "c8BrOjVDnaRn"
   },
   "source": [
    "**Image augmentation and normalization** \n",
    "\n",
    "- Transforms can be chained together using Compose\n",
    "- In image augmentation we randomly flip images, so that our model can detect wrongly oriented images too\n",
    "- All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. \n",
    "- Normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
    "- We first Resize the image to 256 then crop it to 224, so that it doesnt cut important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1622571743002,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "VhsuXv29naRo"
   },
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std)])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1622571819254,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "PNo9oNeQnaRp"
   },
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier'\n",
    "#print(os.listdir(f'{path}/flowers'))\n",
    "data_dir = f'{path}/flowers'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-kqZprnnaRp"
   },
   "source": [
    "A call to ImageFolder(Path, Transform) applies our transformations to all the images in the specified directory.\n",
    "We will create a dictorionary called img_dataset for train and test folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1622571823624,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "LgDzozLlnaRq"
   },
   "outputs": [],
   "source": [
    "img_datasets ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1622572804044,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "MmAxfqf2naRq"
   },
   "outputs": [],
   "source": [
    "# That's how easily you can for images folders in Pytorch for further operations\n",
    "img_datasets['train']= datasets.ImageFolder(data_dir , train_transform)\n",
    "img_datasets['test']= datasets.ImageFolder(data_dir, test_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BanA00o5naRr"
   },
   "source": [
    "Classes Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1622572476012,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "NQ4BCFQSnaRr",
    "outputId": "e697b625-9314-479a-e26c-3db16d910936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ['daisy', 'dandelion', 'flowers', 'rose', 'sunflower', 'tulip']\n",
      "test ['daisy', 'dandelion', 'flowers', 'rose', 'sunflower', 'tulip']\n"
     ]
    }
   ],
   "source": [
    "# these gets extracted from the folder name\n",
    "train_class_names = img_datasets['train'].classes\n",
    "print(\"train\", train_class_names)\n",
    "\n",
    "test_class_names = img_datasets['test'].classes\n",
    "print(\"test\",test_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1622572540381,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "K4-OYuvcnaRr",
    "outputId": "3cb5448d-5718-4737-eb5e-cedd1bf4385a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train {'daisy': 0, 'dandelion': 1, 'flowers': 2, 'rose': 3, 'sunflower': 4, 'tulip': 5}\n",
      "test {'daisy': 0, 'dandelion': 1, 'flowers': 2, 'rose': 3, 'sunflower': 4, 'tulip': 5}\n"
     ]
    }
   ],
   "source": [
    "# these gets extracted from the folder name - class label mapping\n",
    "train_class_idx = img_datasets['train'].class_to_idx\n",
    "print(\"train\",train_class_idx)\n",
    "\n",
    "test_class_idx = img_datasets['test'].class_to_idx\n",
    "print(\"test\",test_class_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNR-LEBSnaRr"
   },
   "source": [
    "Creating Train & Test DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1622572551871,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "cUD52-eMnaRs",
    "outputId": "25083dc9-ee4e-436e-9081-496b8037b0d7"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(img_datasets['train'],\n",
    "                                                   batch_size=10,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(img_datasets['test'],\n",
    "                                                   batch_size=10,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bJ8SrkonaRs"
   },
   "source": [
    "Let's examing a Batch of training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9314,
     "status": "ok",
     "timestamp": 1622572619430,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "vVcG-VoZnaRs",
    "outputId": "74d7b4cd-2a49-44ed-9128-e85d2fec326f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train torch.Size([10, 3, 224, 224])\n",
      "test torch.Size([10, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "train_images , labels = next(iter(train_loader))\n",
    "print(\"train\", train_images.shape)\n",
    "\n",
    "test_images , labels = next(iter(test_loader))\n",
    "print(\"test\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJbtmWA2naRs"
   },
   "source": [
    "- 10 - number of images in a single batch\n",
    "- 3 - number channels \n",
    "- 224 - width & height of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1622572694095,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "zRly1E_jnaRs",
    "outputId": "62555964-a584-494f-bc38-9a443b7d1fdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 0, 0, 2, 2, 2, 3, 3, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at the labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQIiLwbAnaRt"
   },
   "source": [
    "All of the pretrained models are present inside torchvision , in this tutorial we will use vgg16 pretrained layer.\n",
    "PS: In Kaggle to download the pretrained model , you need to set Internet to On in settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "bd21c59d4f8944abadd6a7b4eded7131",
      "3e0f9de1d570411bb46536f3621aafe3",
      "dd0e7b5b8f52462390274cecfdff0956",
      "dc4bf40e9138405abcc54f846db01396",
      "590fac8327ac4271a92b1038fda6a7c0",
      "482a8ddd36cd4b758a7493ded8364743",
      "4621f3bed281497b8b4d5ab33c650beb",
      "ce1fcda5bd564cf6bf17e8ae15a035d7"
     ]
    },
    "executionInfo": {
     "elapsed": 6482,
     "status": "ok",
     "timestamp": 1622572007912,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "Irjhq7IdnaRt",
    "outputId": "d606e61c-4b73-4293-eeb5-2f1f33d52f34"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_6SyqtXnaRt"
   },
   "source": [
    "**Freezing model's layers:**\n",
    "\n",
    "We will freeze all the layers in the network except the final layer.\n",
    "requires_grad == False will freeze the parameters so that the gradients are not computed in backward() i.e. weights of these layers won't be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1622572011138,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "4wtk1Ck0naRt"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.required_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1622572014531,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "th74rAPCnaRt",
    "outputId": "2ea1df6c-6375-4672-fc65-59f4d9e02c20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's check the model archietecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPK_w83mnaRu"
   },
   "source": [
    "If you remember we have five classes i.e. five class image classification , in the above print out if you look closely the (classifier)\n",
    "section - this is doing something else. We need to change the classifier to make it a 5 class classifier.\n",
    "\n",
    "we need to feed the no of input features to the linear layer (classifier[0]) to our newly created linear layer and output would be 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1622572019684,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "z5z_OaQSnaRu",
    "outputId": "0ac4a5fd-78a4-4274-a61a-55c533cc5475"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25088"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_inputs = model.classifier[0].in_features\n",
    "num_of_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1622572027327,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "bWrI4VGgnaRu"
   },
   "outputs": [],
   "source": [
    "# restructaring the classifier\n",
    "import torch.nn as nn\n",
    "model.classifier = nn.Sequential(\n",
    "                      nn.Linear(num_of_inputs, 5),\n",
    "                        nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1622572028907,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "uJE888hknaRu",
    "outputId": "3dd9edd2-5eb9-4ebf-d7e8-0bd16647665a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=5, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's check the model archietecture again to see the changes \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vxbTGaynaRu"
   },
   "source": [
    "Hope you can see the changes in the classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1622572033634,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "GsZ4pDDKnaRv",
    "outputId": "98120ddd-f21b-4696-d924-c95a0bf88a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    CUDA_LAUNCH_BLOCKING=1\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "N0HtRRNKnaRv"
   },
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "aAjhPApYnaRv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.1462,  1.9920,  1.8893,  ...,  1.0502,  1.3584,  1.4954],\n",
      "          [ 2.1804,  1.9920,  1.8550,  ...,  1.3242,  1.4954,  1.6153],\n",
      "          [ 2.1633,  2.0092,  1.8379,  ...,  1.1529,  1.3755,  1.5297],\n",
      "          ...,\n",
      "          [-2.0837, -2.0837, -1.9809,  ...,  1.5982,  1.7180,  1.7694],\n",
      "          [-2.0494, -2.0323, -1.8610,  ...,  1.6324,  1.7009,  1.8037],\n",
      "          [-1.9980, -1.9467, -1.7412,  ...,  1.9064,  1.9064,  2.0434]],\n",
      "\n",
      "         [[-0.2675, -0.6352, -1.0028,  ...,  1.0980,  0.9755,  0.9055],\n",
      "          [-0.3200, -0.5651, -0.9328,  ...,  0.6954,  0.7129,  0.7829],\n",
      "          [-0.4601, -0.4951, -0.8452,  ...,  0.2752,  0.5903,  0.7829],\n",
      "          ...,\n",
      "          [-1.1253, -1.1078, -1.0903,  ..., -1.5980, -1.4755, -1.3880],\n",
      "          [-1.0028, -1.0903, -1.0903,  ..., -1.1954, -0.9853, -0.9153],\n",
      "          [-0.9853, -1.1078, -1.1429,  ..., -0.4076, -0.2325, -0.1625]],\n",
      "\n",
      "         [[ 1.2457,  0.8797,  0.4962,  ..., -1.5430, -1.2467, -0.5321],\n",
      "          [ 1.2108,  0.9319,  0.5659,  ..., -1.6650, -1.5430, -0.9678],\n",
      "          [ 1.1062,  1.0191,  0.6879,  ..., -1.7522, -1.6824, -1.5081],\n",
      "          ...,\n",
      "          [-0.8284, -1.1247, -1.3164,  ...,  0.0953,  0.1999,  0.3045],\n",
      "          [-0.9678, -1.3164, -1.4733,  ...,  0.4439,  0.5311,  0.5834],\n",
      "          [-1.2641, -1.5256, -1.6127,  ...,  1.1062,  1.0888,  1.1237]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8379,  1.8379,  1.8379,  ...,  2.1462,  2.1462,  2.1462],\n",
      "          [ 1.8379,  1.8379,  1.8379,  ...,  2.1462,  2.1462,  2.1462],\n",
      "          [ 1.8208,  1.8208,  1.8208,  ...,  2.1462,  2.1290,  2.1290],\n",
      "          ...,\n",
      "          [ 1.6324,  1.6153,  1.5982,  ...,  1.6838,  1.7523,  1.7694],\n",
      "          [ 1.6153,  1.5982,  1.5810,  ...,  1.7180,  1.7694,  1.7865],\n",
      "          [ 1.6153,  1.5982,  1.5810,  ...,  1.7352,  1.7865,  1.8037]],\n",
      "\n",
      "         [[ 1.2206,  1.2206,  1.2031,  ...,  1.8683,  1.8508,  1.8508],\n",
      "          [ 1.2206,  1.2206,  1.2031,  ...,  1.8683,  1.8508,  1.8508],\n",
      "          [ 1.2206,  1.2206,  1.2206,  ...,  1.8683,  1.8508,  1.8333],\n",
      "          ...,\n",
      "          [ 0.4853,  0.4853,  0.5028,  ...,  1.7458,  1.7283,  1.7283],\n",
      "          [ 0.4678,  0.4678,  0.4853,  ...,  1.7808,  1.7633,  1.7458],\n",
      "          [ 0.4503,  0.4678,  0.4853,  ...,  1.7983,  1.7808,  1.7633]],\n",
      "\n",
      "         [[ 1.4897,  1.4897,  1.4897,  ...,  2.0997,  2.0823,  2.0823],\n",
      "          [ 1.4897,  1.4897,  1.4897,  ...,  2.1171,  2.0823,  2.0823],\n",
      "          [ 1.4897,  1.4897,  1.4897,  ...,  2.1346,  2.0997,  2.0997],\n",
      "          ...,\n",
      "          [ 0.8448,  0.8448,  0.8274,  ...,  1.1411,  1.2980,  1.3677],\n",
      "          [ 0.8448,  0.8448,  0.8274,  ...,  1.1237,  1.2805,  1.3502],\n",
      "          [ 0.8448,  0.8448,  0.8274,  ...,  1.1062,  1.2805,  1.3502]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2967,  0.3309,  0.6049,  ..., -0.1143, -0.0801, -0.0801],\n",
      "          [ 0.2967,  0.3309,  0.6049,  ..., -0.1143, -0.0801, -0.0801],\n",
      "          [ 0.2967,  0.3309,  0.5878,  ..., -0.0972, -0.0629, -0.0629],\n",
      "          ...,\n",
      "          [ 1.9920,  1.9920,  1.9920,  ..., -0.2684, -0.3027, -0.3027],\n",
      "          [ 1.9920,  1.9920,  1.9920,  ..., -0.2684, -0.3027, -0.3027],\n",
      "          [ 1.9920,  1.9920,  1.9920,  ..., -0.2684, -0.3027, -0.3027]],\n",
      "\n",
      "         [[ 0.5028,  0.5203,  0.6604,  ..., -0.1800, -0.1275, -0.1275],\n",
      "          [ 0.5028,  0.5203,  0.6604,  ..., -0.1800, -0.1275, -0.1275],\n",
      "          [ 0.5028,  0.5203,  0.6604,  ..., -0.1800, -0.1275, -0.1275],\n",
      "          ...,\n",
      "          [ 2.1835,  2.1835,  2.1835,  ..., -0.1450, -0.1450, -0.1450],\n",
      "          [ 2.1835,  2.1835,  2.1835,  ..., -0.1450, -0.1450, -0.1450],\n",
      "          [ 2.1835,  2.1835,  2.1835,  ..., -0.1450, -0.1450, -0.1450]],\n",
      "\n",
      "         [[ 0.6531,  0.6531,  0.6356,  ..., -0.4798, -0.4275, -0.4275],\n",
      "          [ 0.6531,  0.6531,  0.6356,  ..., -0.4798, -0.4275, -0.4275],\n",
      "          [ 0.6705,  0.6705,  0.6705,  ..., -0.4798, -0.4275, -0.4275],\n",
      "          ...,\n",
      "          [ 2.3437,  2.3437,  2.3437,  ..., -0.7238, -0.7413, -0.7413],\n",
      "          [ 2.3437,  2.3437,  2.3437,  ..., -0.7238, -0.7238, -0.7238],\n",
      "          [ 2.3437,  2.3437,  2.3437,  ..., -0.7238, -0.7238, -0.7238]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.7754, -1.7412, -1.7069,  ..., -0.1143, -0.0972, -0.0629],\n",
      "          [-1.7583, -1.7412, -1.7069,  ..., -0.0801, -0.0801, -0.0458],\n",
      "          [-1.7412, -1.7240, -1.7069,  ..., -0.0458, -0.0458, -0.0287],\n",
      "          ...,\n",
      "          [ 1.8379,  1.8722,  1.9064,  ...,  1.5125,  1.5468,  1.5810],\n",
      "          [ 1.8379,  1.8893,  1.9235,  ...,  1.5125,  1.5468,  1.5639],\n",
      "          [ 1.8379,  1.8893,  1.9235,  ...,  1.5125,  1.5468,  1.5639]],\n",
      "\n",
      "         [[-1.7031, -1.7031, -1.6856,  ..., -0.4426, -0.4251, -0.3901],\n",
      "          [-1.7031, -1.7031, -1.6856,  ..., -0.4251, -0.4076, -0.3901],\n",
      "          [-1.7206, -1.7206, -1.7031,  ..., -0.3901, -0.3901, -0.3725],\n",
      "          ...,\n",
      "          [ 1.8508,  1.8683,  1.9034,  ...,  1.2556,  1.3431,  1.3957],\n",
      "          [ 1.8333,  1.8683,  1.9034,  ...,  1.2906,  1.3606,  1.4132],\n",
      "          [ 1.8333,  1.8683,  1.9034,  ...,  1.3081,  1.3782,  1.4132]],\n",
      "\n",
      "         [[-1.8044, -1.7870, -1.7696,  ...,  1.0539,  1.0714,  1.1062],\n",
      "          [-1.8044, -1.7870, -1.7696,  ...,  1.1062,  1.1062,  1.1237],\n",
      "          [-1.7870, -1.7696, -1.7522,  ...,  1.1585,  1.1585,  1.1411],\n",
      "          ...,\n",
      "          [-1.7696, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7696],\n",
      "          [-1.7870, -1.7522, -1.7347,  ..., -1.7696, -1.7696, -1.7870],\n",
      "          [-1.7870, -1.7522, -1.7173,  ..., -1.7696, -1.7696, -1.7870]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9817,  0.9646,  0.9817,  ..., -0.5938, -0.5767, -0.5596],\n",
      "          [ 0.9817,  0.9646,  0.9646,  ..., -0.6109, -0.5938, -0.5596],\n",
      "          [ 0.9646,  0.9474,  0.9474,  ..., -0.5938, -0.5767, -0.5424],\n",
      "          ...,\n",
      "          [-1.2274, -1.1247, -1.1247,  ..., -1.6555, -1.7240, -0.9877],\n",
      "          [-1.1760, -1.1075, -1.1589,  ..., -1.4672, -1.6042, -1.2103],\n",
      "          [-1.0904, -1.1247, -1.2103,  ..., -1.2788, -1.3815, -1.3473]],\n",
      "\n",
      "         [[ 1.4482,  1.4132,  1.4132,  ...,  0.7829,  0.7654,  0.7479],\n",
      "          [ 1.4307,  1.3957,  1.3957,  ...,  0.7479,  0.7479,  0.7654],\n",
      "          [ 1.3957,  1.3606,  1.3606,  ...,  0.7304,  0.7479,  0.7829],\n",
      "          ...,\n",
      "          [-0.6176, -0.5126, -0.5126,  ..., -1.1779, -1.2129, -0.8277],\n",
      "          [-0.5651, -0.4951, -0.5476,  ..., -0.9678, -1.0728, -1.0203],\n",
      "          [-0.4776, -0.5126, -0.6001,  ..., -0.7752, -0.8277, -1.1779]],\n",
      "\n",
      "         [[ 1.9080,  1.9080,  1.9080,  ...,  2.5006,  2.4831,  2.4483],\n",
      "          [ 1.9080,  1.9254,  1.9254,  ...,  2.4831,  2.4657,  2.4657],\n",
      "          [ 1.8905,  1.9080,  1.9080,  ...,  2.4657,  2.4483,  2.4483],\n",
      "          ...,\n",
      "          [-1.3861, -1.2990, -1.2990,  ..., -1.6302, -1.6302, -1.1073],\n",
      "          [-1.3513, -1.2816, -1.3339,  ..., -1.5779, -1.5430, -1.1770],\n",
      "          [-1.2641, -1.2990, -1.3861,  ..., -1.5081, -1.3861, -1.2990]]],\n",
      "\n",
      "\n",
      "        [[[-0.3198, -0.7308, -0.9705,  ..., -0.0972, -0.2513, -0.3712],\n",
      "          [ 0.1768, -0.2856, -0.6281,  ..., -0.1828, -0.2684, -0.2856],\n",
      "          [ 0.7248,  0.3309, -0.0801,  ..., -0.1486, -0.1486, -0.0629],\n",
      "          ...,\n",
      "          [ 0.4851,  0.4679,  0.4851,  ..., -0.6281, -0.5253, -0.3883],\n",
      "          [ 0.4851,  0.4679,  0.4851,  ..., -0.6965, -0.6109, -0.4739],\n",
      "          [ 0.5022,  0.4679,  0.4851,  ..., -0.7650, -0.6794, -0.5596]],\n",
      "\n",
      "         [[ 0.2577, -0.1450, -0.3901,  ...,  0.1176, -0.0574, -0.2325],\n",
      "          [ 0.6779,  0.2227, -0.1099,  ...,  0.0301, -0.0924, -0.1450],\n",
      "          [ 1.1856,  0.8179,  0.3978,  ...,  0.0826,  0.0301,  0.0826],\n",
      "          ...,\n",
      "          [ 0.0126,  0.0301,  0.1001,  ...,  0.1877,  0.1527,  0.1352],\n",
      "          [ 0.0126,  0.0301,  0.1001,  ...,  0.0826,  0.0651,  0.0651],\n",
      "          [-0.0049,  0.0126,  0.0826,  ..., -0.0224, -0.0224, -0.0049]],\n",
      "\n",
      "         [[-0.6193, -0.7761, -0.8284,  ..., -0.2010, -0.3753, -0.4973],\n",
      "          [-0.4973, -0.6890, -0.7587,  ..., -0.3927, -0.5321, -0.5844],\n",
      "          [-0.3404, -0.5147, -0.6367,  ..., -0.5321, -0.6018, -0.6018],\n",
      "          ...,\n",
      "          [-1.5430, -1.5430, -1.4733,  ...,  0.0256,  0.0605,  0.0953],\n",
      "          [-1.5430, -1.5430, -1.4733,  ..., -0.0790, -0.0267,  0.0256],\n",
      "          [-1.5256, -1.5256, -1.4733,  ..., -0.1661, -0.1138, -0.0441]]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-76eba473b62d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_on_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mCUDA_LAUNCH_BLOCKING\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# clear the gradients of all optimized variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        print(data)\n",
    "        if train_on_gpu:\n",
    "            CUDA_LAUNCH_BLOCKING=1\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        #calculate accuracy\n",
    "        ps = torch.exp(output)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == target.view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    \n",
    "# calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch+1, \n",
    "            train_loss\n",
    "            ))\n",
    "    print(f\"Train accuracy: {train_accuracy/len(train_loader):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "02w-KAzunaRw"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d8a9e35dadae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_on_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "# Checking Test Performence\n",
    "test_accuracy = 0\n",
    "model.eval() # prep model for evaluation\n",
    "for data, target in test_loader:\n",
    "    if train_on_gpu:\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    #calculate accuracy\n",
    "    ps = torch.exp(output)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == target.view(*top_class.shape)\n",
    "    test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy/len(test_loader):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGPQxxuvnaRw"
   },
   "source": [
    "Accuracy can be improved by changing the classifer archietecture !! "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch-flower-classification-transfer-learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3e0f9de1d570411bb46536f3621aafe3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4621f3bed281497b8b4d5ab33c650beb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "482a8ddd36cd4b758a7493ded8364743": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "590fac8327ac4271a92b1038fda6a7c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bd21c59d4f8944abadd6a7b4eded7131": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd0e7b5b8f52462390274cecfdff0956",
       "IPY_MODEL_dc4bf40e9138405abcc54f846db01396"
      ],
      "layout": "IPY_MODEL_3e0f9de1d570411bb46536f3621aafe3"
     }
    },
    "ce1fcda5bd564cf6bf17e8ae15a035d7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc4bf40e9138405abcc54f846db01396": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce1fcda5bd564cf6bf17e8ae15a035d7",
      "placeholder": "​",
      "style": "IPY_MODEL_4621f3bed281497b8b4d5ab33c650beb",
      "value": " 528M/528M [07:16&lt;00:00, 1.27MB/s]"
     }
    },
    "dd0e7b5b8f52462390274cecfdff0956": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_482a8ddd36cd4b758a7493ded8364743",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_590fac8327ac4271a92b1038fda6a7c0",
      "value": 553433881
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
