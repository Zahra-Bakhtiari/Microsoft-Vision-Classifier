{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"pytorch-flower-classification-transfer-learning.ipynb","provenance":[],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"bd21c59d4f8944abadd6a7b4eded7131":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3e0f9de1d570411bb46536f3621aafe3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dd0e7b5b8f52462390274cecfdff0956","IPY_MODEL_dc4bf40e9138405abcc54f846db01396"]}},"3e0f9de1d570411bb46536f3621aafe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dd0e7b5b8f52462390274cecfdff0956":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_590fac8327ac4271a92b1038fda6a7c0","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":553433881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":553433881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_482a8ddd36cd4b758a7493ded8364743"}},"dc4bf40e9138405abcc54f846db01396":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4621f3bed281497b8b4d5ab33c650beb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 528M/528M [07:16&lt;00:00, 1.27MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce1fcda5bd564cf6bf17e8ae15a035d7"}},"590fac8327ac4271a92b1038fda6a7c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"482a8ddd36cd4b758a7493ded8364743":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4621f3bed281497b8b4d5ab33c650beb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ce1fcda5bd564cf6bf17e8ae15a035d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WxbH_yJKzpMB","executionInfo":{"status":"ok","timestamp":1622571688986,"user_tz":420,"elapsed":22620,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"2869249b-2fa4-4b38-862a-8d38055719a2"},"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n","FOLDERNAME = 'cs231n/assignments/project/'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# This downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/flowers/\n","!bash get_datasets.sh\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/cs231n/assignments/project/flowers\n","bash: get_datasets.sh: No such file or directory\n","/content/drive/My Drive/cs231n/assignments/project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"j131GT4znaRk","executionInfo":{"status":"ok","timestamp":1622571736570,"user_tz":420,"elapsed":3048,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"4f8ec348-08da-4d76-d7a8-62a7491b18a4"},"source":["import torch\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","\n","import os\n","path = '/content/drive/My Drive/{}'.format(FOLDERNAME)\n","print(os.listdir(f'{path}/flowers'))\n","\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":2,"outputs":[{"output_type":"stream","text":["['flowers', 'rose', 'dandelion', 'daisy', 'sunflower', 'tulip']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false,"id":"c8BrOjVDnaRn"},"source":["**Image augmentation and normalization** \n","\n","- Transforms can be chained together using Compose\n","- In image augmentation we randomly flip images, so that our model can detect wrongly oriented images too\n","- All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. \n","- Normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n","- We first Resize the image to 256 then crop it to 224, so that it doesnt cut important features"]},{"cell_type":"code","metadata":{"trusted":true,"id":"VhsuXv29naRo","executionInfo":{"status":"ok","timestamp":1622571743002,"user_tz":420,"elapsed":129,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}}},"source":["mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","train_transform = transforms.Compose([\n","                                transforms.Resize(256),\n","                                transforms.RandomResizedCrop(224),\n","                                transforms.RandomHorizontalFlip(),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(mean, std)])\n","\n","test_transform = transforms.Compose([\n","                                transforms.Resize(256),\n","                                transforms.CenterCrop(224),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(mean, std)])"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"PNo9oNeQnaRp","executionInfo":{"status":"ok","timestamp":1622571819254,"user_tz":420,"elapsed":74,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}}},"source":["path = '/content/drive/My Drive/{}'.format(FOLDERNAME)\n","#print(os.listdir(f'{path}/flowers'))\n","data_dir = f'{path}/flowers'\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c-kqZprnnaRp"},"source":["A call to ImageFolder(Path, Transform) applies our transformations to all the images in the specified directory.\n","We will create a dictorionary called img_dataset for train and test folder**"]},{"cell_type":"code","metadata":{"trusted":true,"id":"LgDzozLlnaRq","executionInfo":{"status":"ok","timestamp":1622571823624,"user_tz":420,"elapsed":74,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}}},"source":["img_datasets ={}"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"MmAxfqf2naRq","executionInfo":{"status":"ok","timestamp":1622572804044,"user_tz":420,"elapsed":339,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}}},"source":["# That's how easily you can for images folders in Pytorch for further operations\n","img_datasets['train']= datasets.ImageFolder(data_dir , train_transform)\n","img_datasets['test']= datasets.ImageFolder(data_dir, test_transform)\n"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BanA00o5naRr"},"source":["Classes Present"]},{"cell_type":"code","metadata":{"trusted":true,"id":"NQ4BCFQSnaRr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622572476012,"user_tz":420,"elapsed":85,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"e697b625-9314-479a-e26c-3db16d910936"},"source":["# these gets extracted from the folder name\n","train_class_names = img_datasets['train'].classes\n","print(\"train\", train_class_names)\n","\n","test_class_names = img_datasets['test'].classes\n","print(\"test\",test_class_names)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["train ['daisy', 'dandelion', 'flowers', 'rose', 'sunflower', 'tulip']\n","test ['daisy', 'dandelion', 'flowers', 'rose', 'sunflower', 'tulip']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"K4-OYuvcnaRr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622572540381,"user_tz":420,"elapsed":98,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"3cb5448d-5718-4737-eb5e-cedd1bf4385a"},"source":["# these gets extracted from the folder name - class label mapping\n","train_class_idx = img_datasets['train'].class_to_idx\n","print(\"train\",train_class_idx)\n","\n","test_class_idx = img_datasets['test'].class_to_idx\n","print(\"test\",test_class_idx)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["train {'daisy': 0, 'dandelion': 1, 'flowers': 2, 'rose': 3, 'sunflower': 4, 'tulip': 5}\n","test {'daisy': 0, 'dandelion': 1, 'flowers': 2, 'rose': 3, 'sunflower': 4, 'tulip': 5}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oNR-LEBSnaRr"},"source":["Creating Train & Test DataLoaders"]},{"cell_type":"code","metadata":{"trusted":true,"id":"cUD52-eMnaRs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622572551871,"user_tz":420,"elapsed":84,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"25083dc9-ee4e-436e-9081-496b8037b0d7"},"source":["train_loader = torch.utils.data.DataLoader(img_datasets['train'],\n","                                                   batch_size=10,\n","                                                   shuffle=True,\n","                                                   num_workers=4)\n","\n","test_loader = torch.utils.data.DataLoader(img_datasets['test'],\n","                                                   batch_size=10,\n","                                                   shuffle=True,\n","                                                   num_workers=4)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"9bJ8SrkonaRs"},"source":["Let's examing a Batch of training Data"]},{"cell_type":"code","metadata":{"trusted":true,"id":"vVcG-VoZnaRs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622572619430,"user_tz":420,"elapsed":9314,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"74d7b4cd-2a49-44ed-9128-e85d2fec326f"},"source":["train_images , labels = next(iter(train_loader))\n","print(\"train\", train_images.shape)\n","\n","test_images , labels = next(iter(test_loader))\n","print(\"test\", test_images.shape)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["train torch.Size([10, 3, 224, 224])\n","test torch.Size([10, 3, 224, 224])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hJbtmWA2naRs"},"source":["- 10 - number of images in a single batch\n","- 3 - number channels \n","- 224 - width & height of the image"]},{"cell_type":"code","metadata":{"trusted":true,"id":"zRly1E_jnaRs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622572694095,"user_tz":420,"elapsed":79,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"62555964-a584-494f-bc38-9a443b7d1fdf"},"source":["# lets look at the labels\n","labels"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 2, 1, 2, 2, 2, 2, 1, 2, 5])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"dQIiLwbAnaRt"},"source":["All of the pretrained models are present inside torchvision , in this tutorial we will use vgg16 pretrained layer.\n","PS: In Kaggle to download the pretrained model , you need to set Internet to On in settings."]},{"cell_type":"code","metadata":{"trusted":true,"id":"Irjhq7IdnaRt","colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["bd21c59d4f8944abadd6a7b4eded7131","3e0f9de1d570411bb46536f3621aafe3","dd0e7b5b8f52462390274cecfdff0956","dc4bf40e9138405abcc54f846db01396","590fac8327ac4271a92b1038fda6a7c0","482a8ddd36cd4b758a7493ded8364743","4621f3bed281497b8b4d5ab33c650beb","ce1fcda5bd564cf6bf17e8ae15a035d7"]},"executionInfo":{"status":"ok","timestamp":1622572007912,"user_tz":420,"elapsed":6482,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"d606e61c-4b73-4293-eeb5-2f1f33d52f34"},"source":["import torchvision.models as models\n","\n","model = models.vgg16(pretrained=True)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd21c59d4f8944abadd6a7b4eded7131","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T_6SyqtXnaRt"},"source":["**Freezing model's layers:**\n","\n","We will freeze all the layers in the network except the final layer.\n","requires_grad == False will freeze the parameters so that the gradients are not computed in backward() i.e. weights of these layers won't be trained"]},{"cell_type":"code","metadata":{"trusted":true,"id":"4wtk1Ck0naRt","executionInfo":{"status":"ok","timestamp":1622572011138,"user_tz":420,"elapsed":5,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}}},"source":["for param in model.parameters():\n","    param.required_grad = False"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"th74rAPCnaRt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622572014531,"user_tz":420,"elapsed":6,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"2ea1df6c-6375-4672-fc65-59f4d9e02c20"},"source":["# Now let's check the model archietecture\n","model"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"iPK_w83mnaRu"},"source":["If you remember we have five classes i.e. five class image classification , in the above print out if you look closely the (classifier)\n","section - this is doing something else. We need to change the classifier to make it a 5 class classifier.\n","\n","we need to feed the no of input features to the linear layer (classifier[0]) to our newly created linear layer and output would be 5."]},{"cell_type":"code","metadata":{"trusted":true,"id":"z5z_OaQSnaRu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622572019684,"user_tz":420,"elapsed":11,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"0ac4a5fd-78a4-4274-a61a-55c533cc5475"},"source":["num_of_inputs = model.classifier[0].in_features\n","num_of_inputs"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25088"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"trusted":true,"id":"bWrI4VGgnaRu","executionInfo":{"status":"ok","timestamp":1622572027327,"user_tz":420,"elapsed":79,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}}},"source":["# restructaring the classifier\n","import torch.nn as nn\n","model.classifier = nn.Sequential(\n","                      nn.Linear(num_of_inputs, 5),\n","                        nn.LogSoftmax(dim=1))"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"uJE888hknaRu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622572028907,"user_tz":420,"elapsed":8,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"3dd9edd2-5eb9-4ebf-d7e8-0bd16647665a"},"source":["# Now let's check the model archietecture again to see the changes \n","model"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=5, bias=True)\n","    (1): LogSoftmax(dim=1)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"5vxbTGaynaRu"},"source":["Hope you can see the changes in the classifier layer"]},{"cell_type":"code","metadata":{"trusted":true,"id":"GsZ4pDDKnaRv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622572033634,"user_tz":420,"elapsed":84,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"98120ddd-f21b-4696-d924-c95a0bf88a87"},"source":["# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')\n","# move tensors to GPU if CUDA is available\n","if train_on_gpu:\n","    model.cuda()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["CUDA is not available.  Training on CPU ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"N0HtRRNKnaRv"},"source":["# loss function and optimizer\n","criterion = nn.NLLLoss()\n","optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"aAjhPApYnaRv"},"source":["# number of epochs to train the model\n","n_epochs = 10\n","\n","\n","for epoch in range(n_epochs):\n","    # monitor training loss\n","    train_loss = 0.0\n","    train_accuracy = 0\n","    \n","    ###################\n","    # train the model #\n","    ###################\n","    model.train() # prep model for training\n","    for data, target in train_loader:\n","        if train_on_gpu:\n","            data, target = Variable(data.cuda()), Variable(target.cuda())\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the loss\n","        loss = criterion(output, target)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update running training loss\n","        train_loss += loss.item()*data.size(0)\n","        #calculate accuracy\n","        ps = torch.exp(output)\n","        top_p, top_class = ps.topk(1, dim=1)\n","        equals = top_class == target.view(*top_class.shape)\n","        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","    \n","# calculate average loss over an epoch\n","    train_loss = train_loss/len(train_loader.dataset)\n","\n","    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n","            epoch+1, \n","            train_loss\n","            ))\n","    print(f\"Train accuracy: {train_accuracy/len(train_loader):.3f}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"02w-KAzunaRw"},"source":["# Checking Test Performence\n","test_accuracy = 0\n","model.eval() # prep model for evaluation\n","for data, target in test_loader:\n","    if train_on_gpu:\n","        data, target = Variable(data.cuda()), Variable(target.cuda())\n","    # forward pass: compute predicted outputs by passing inputs to the model\n","    output = model(data)\n","    # calculate the loss\n","    loss = criterion(output, target)\n","    #calculate accuracy\n","    ps = torch.exp(output)\n","    top_p, top_class = ps.topk(1, dim=1)\n","    equals = top_class == target.view(*top_class.shape)\n","    test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","print(f\"Test accuracy: {test_accuracy/len(test_loader):.3f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FGPQxxuvnaRw"},"source":["Accuracy can be improved by changing the classifer archietecture !! "]}]}