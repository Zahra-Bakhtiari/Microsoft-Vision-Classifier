{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22620,
     "status": "ok",
     "timestamp": 1622571688986,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "WxbH_yJKzpMB",
    "outputId": "2869249b-2fa4-4b38-862a-8d38055719a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier/flowers\n",
      "bash: get_datasets.sh: No such file or directory\n",
      "/home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "FOLDERNAME = 'home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier/'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier')\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd /$FOLDERNAME/flowers/\n",
    "!bash get_datasets.sh\n",
    "%cd /$FOLDERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3048,
     "status": "ok",
     "timestamp": 1622571736570,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "j131GT4znaRk",
    "outputId": "4f8ec348-08da-4d76-d7a8-62a7491b18a4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-56cd14c8b68d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "path = '/home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier'\n",
    "print(os.listdir(f'{path}/flowers'))\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "id": "c8BrOjVDnaRn"
   },
   "source": [
    "**Image augmentation and normalization** \n",
    "\n",
    "- Transforms can be chained together using Compose\n",
    "- In image augmentation we randomly flip images, so that our model can detect wrongly oriented images too\n",
    "- All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. \n",
    "- Normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
    "- We first Resize the image to 256 then crop it to 224, so that it doesnt cut important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1622571743002,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "VhsuXv29naRo"
   },
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std)])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1622571819254,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "PNo9oNeQnaRp"
   },
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/Vision-Classifiers/Microsoft-Vision-Classifier'\n",
    "#print(os.listdir(f'{path}/flowers'))\n",
    "data_dir = f'{path}/flowers'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-kqZprnnaRp"
   },
   "source": [
    "A call to ImageFolder(Path, Transform) applies our transformations to all the images in the specified directory.\n",
    "We will create a dictorionary called img_dataset for train and test folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1622571823624,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "LgDzozLlnaRq"
   },
   "outputs": [],
   "source": [
    "img_datasets ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1622572804044,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "MmAxfqf2naRq"
   },
   "outputs": [],
   "source": [
    "# That's how easily you can for images folders in Pytorch for further operations\n",
    "img_datasets['train']= datasets.ImageFolder(data_dir , train_transform)\n",
    "img_datasets['test']= datasets.ImageFolder(data_dir, test_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BanA00o5naRr"
   },
   "source": [
    "Classes Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1622572476012,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "NQ4BCFQSnaRr",
    "outputId": "e697b625-9314-479a-e26c-3db16d910936"
   },
   "outputs": [],
   "source": [
    "# these gets extracted from the folder name\n",
    "train_class_names = img_datasets['train'].classes\n",
    "print(\"train\", train_class_names)\n",
    "\n",
    "test_class_names = img_datasets['test'].classes\n",
    "print(\"test\",test_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1622572540381,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "K4-OYuvcnaRr",
    "outputId": "3cb5448d-5718-4737-eb5e-cedd1bf4385a"
   },
   "outputs": [],
   "source": [
    "# these gets extracted from the folder name - class label mapping\n",
    "train_class_idx = img_datasets['train'].class_to_idx\n",
    "print(\"train\",train_class_idx)\n",
    "\n",
    "test_class_idx = img_datasets['test'].class_to_idx\n",
    "print(\"test\",test_class_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNR-LEBSnaRr"
   },
   "source": [
    "Creating Train & Test DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1622572551871,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "cUD52-eMnaRs",
    "outputId": "25083dc9-ee4e-436e-9081-496b8037b0d7"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(img_datasets['train'],\n",
    "                                                   batch_size=10,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(img_datasets['test'],\n",
    "                                                   batch_size=10,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bJ8SrkonaRs"
   },
   "source": [
    "Let's examing a Batch of training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9314,
     "status": "ok",
     "timestamp": 1622572619430,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "vVcG-VoZnaRs",
    "outputId": "74d7b4cd-2a49-44ed-9128-e85d2fec326f"
   },
   "outputs": [],
   "source": [
    "train_images , labels = next(iter(train_loader))\n",
    "print(\"train\", train_images.shape)\n",
    "\n",
    "test_images , labels = next(iter(test_loader))\n",
    "print(\"test\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJbtmWA2naRs"
   },
   "source": [
    "- 10 - number of images in a single batch\n",
    "- 3 - number channels \n",
    "- 224 - width & height of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1622572694095,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "zRly1E_jnaRs",
    "outputId": "62555964-a584-494f-bc38-9a443b7d1fdf"
   },
   "outputs": [],
   "source": [
    "# lets look at the labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQIiLwbAnaRt"
   },
   "source": [
    "All of the pretrained models are present inside torchvision , in this tutorial we will use vgg16 pretrained layer.\n",
    "PS: In Kaggle to download the pretrained model , you need to set Internet to On in settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "bd21c59d4f8944abadd6a7b4eded7131",
      "3e0f9de1d570411bb46536f3621aafe3",
      "dd0e7b5b8f52462390274cecfdff0956",
      "dc4bf40e9138405abcc54f846db01396",
      "590fac8327ac4271a92b1038fda6a7c0",
      "482a8ddd36cd4b758a7493ded8364743",
      "4621f3bed281497b8b4d5ab33c650beb",
      "ce1fcda5bd564cf6bf17e8ae15a035d7"
     ]
    },
    "executionInfo": {
     "elapsed": 6482,
     "status": "ok",
     "timestamp": 1622572007912,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "Irjhq7IdnaRt",
    "outputId": "d606e61c-4b73-4293-eeb5-2f1f33d52f34"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_6SyqtXnaRt"
   },
   "source": [
    "**Freezing model's layers:**\n",
    "\n",
    "We will freeze all the layers in the network except the final layer.\n",
    "requires_grad == False will freeze the parameters so that the gradients are not computed in backward() i.e. weights of these layers won't be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1622572011138,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "4wtk1Ck0naRt"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.required_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1622572014531,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "th74rAPCnaRt",
    "outputId": "2ea1df6c-6375-4672-fc65-59f4d9e02c20"
   },
   "outputs": [],
   "source": [
    "# Now let's check the model archietecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPK_w83mnaRu"
   },
   "source": [
    "If you remember we have five classes i.e. five class image classification , in the above print out if you look closely the (classifier)\n",
    "section - this is doing something else. We need to change the classifier to make it a 5 class classifier.\n",
    "\n",
    "we need to feed the no of input features to the linear layer (classifier[0]) to our newly created linear layer and output would be 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1622572019684,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "z5z_OaQSnaRu",
    "outputId": "0ac4a5fd-78a4-4274-a61a-55c533cc5475"
   },
   "outputs": [],
   "source": [
    "num_of_inputs = model.classifier[0].in_features\n",
    "num_of_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1622572027327,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "bWrI4VGgnaRu"
   },
   "outputs": [],
   "source": [
    "# restructaring the classifier\n",
    "import torch.nn as nn\n",
    "model.classifier = nn.Sequential(\n",
    "                      nn.Linear(num_of_inputs, 5),\n",
    "                        nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1622572028907,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "uJE888hknaRu",
    "outputId": "3dd9edd2-5eb9-4ebf-d7e8-0bd16647665a"
   },
   "outputs": [],
   "source": [
    "# Now let's check the model archietecture again to see the changes \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vxbTGaynaRu"
   },
   "source": [
    "Hope you can see the changes in the classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1622572033634,
     "user": {
      "displayName": "Mehrnaz B",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64",
      "userId": "14896880010903859463"
     },
     "user_tz": 420
    },
    "id": "GsZ4pDDKnaRv",
    "outputId": "98120ddd-f21b-4696-d924-c95a0bf88a87"
   },
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    CUDA_LAUNCH_BLOCKING=1\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0HtRRNKnaRv"
   },
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAjhPApYnaRv"
   },
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        print(data)\n",
    "        if train_on_gpu:\n",
    "            CUDA_LAUNCH_BLOCKING=1\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        #calculate accuracy\n",
    "        ps = torch.exp(output)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == target.view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    \n",
    "# calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch+1, \n",
    "            train_loss\n",
    "            ))\n",
    "    print(f\"Train accuracy: {train_accuracy/len(train_loader):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02w-KAzunaRw"
   },
   "outputs": [],
   "source": [
    "# Checking Test Performence\n",
    "test_accuracy = 0\n",
    "model.eval() # prep model for evaluation\n",
    "for data, target in test_loader:\n",
    "    if train_on_gpu:\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    #calculate accuracy\n",
    "    ps = torch.exp(output)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == target.view(*top_class.shape)\n",
    "    test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy/len(test_loader):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGPQxxuvnaRw"
   },
   "source": [
    "Accuracy can be improved by changing the classifer archietecture !! "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch-flower-classification-transfer-learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36]",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3e0f9de1d570411bb46536f3621aafe3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4621f3bed281497b8b4d5ab33c650beb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "482a8ddd36cd4b758a7493ded8364743": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "590fac8327ac4271a92b1038fda6a7c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bd21c59d4f8944abadd6a7b4eded7131": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd0e7b5b8f52462390274cecfdff0956",
       "IPY_MODEL_dc4bf40e9138405abcc54f846db01396"
      ],
      "layout": "IPY_MODEL_3e0f9de1d570411bb46536f3621aafe3"
     }
    },
    "ce1fcda5bd564cf6bf17e8ae15a035d7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc4bf40e9138405abcc54f846db01396": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce1fcda5bd564cf6bf17e8ae15a035d7",
      "placeholder": "​",
      "style": "IPY_MODEL_4621f3bed281497b8b4d5ab33c650beb",
      "value": " 528M/528M [07:16&lt;00:00, 1.27MB/s]"
     }
    },
    "dd0e7b5b8f52462390274cecfdff0956": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_482a8ddd36cd4b758a7493ded8364743",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_590fac8327ac4271a92b1038fda6a7c0",
      "value": 553433881
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
