{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"a-comprehensive-guide-to-transfer-learning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"cells":[{"cell_type":"markdown","metadata":{"_uuid":"3b61a89492cdbed65fb1ceff914b9fd4f75c81ac","id":"Ka2OegZ3tcgn"},"source":["## A Comprehensive Guide to Transfer Learning \n","\n","In this kernel I have demonstrated the general techniques that can be used with Transfer Learning. \n","\n","For this kernel I have used the **Flower Recognition** dataset but the basic TL principles remains the same.\n","\n","**Basically , you need to watch two things**\n","\n","**1) The simalarity of your dataset with that of the pre-trained model and **\n","\n","**2) The amount of the data that you have.**\n","\n","Depending on these two conditions you can choose to either fine tune the weights or just train a classifier on top of the pre-trained model."]},{"cell_type":"markdown","metadata":{"_uuid":"5807aead695b00a404422c68571c74c08fda1c65","id":"gOR2bm2Stcgq"},"source":["## [ Please star / upvote if you like the kernel. ]"]},{"cell_type":"markdown","metadata":{"_uuid":"edc9dc1fab5d6e7f7b7eb55beff4c4b6eda504f3","id":"HLaYr2YMtcgq"},"source":["## CONTENTS ::->¶"]},{"cell_type":"markdown","metadata":{"_uuid":"b7ae07e196045117a1dd89921e943fd9b0295838","id":"bAD0FEKMtcgq"},"source":["[ **1 ) Importing Various Modules**](#content1)"]},{"cell_type":"markdown","metadata":{"_uuid":"424911f54eb33b22ab285672a9379a5c97b534fb","id":"koiOSYWutcgr"},"source":["[ **2 ) Preparing the Data**](#content2)"]},{"cell_type":"markdown","metadata":{"_uuid":"9c2ade7ceb8b950eb65b7ecadc24c294b459d111","id":"-5gVpML9tcgr"},"source":["[ **3 ) Modelling**](#content3)"]},{"cell_type":"markdown","metadata":{"_uuid":"9b9d83431a7e347418e83238b3938cf4162aa70b","id":"ET9SZDtqtcgr"},"source":["[ **4 ) Visualizing Predictons on the Validation Set**](#content4)"]},{"cell_type":"markdown","metadata":{"_uuid":"77c9b4cc1efaa0a4544f0e8ae2844264df1f0e90","id":"-6EZI15Ctcgr"},"source":["<a id=\"content1\"></a>\n","## 1 ) Importing Various Modules."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7eQSqoQuYgg","executionInfo":{"status":"ok","timestamp":1622586402138,"user_tz":420,"elapsed":184,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"efd9f857-657f-4851-e1aa-fbe208c9538f"},"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n","FOLDERNAME = 'cs231n/assignments/project/'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# This downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/flowers/\n","!bash get_datasets.sh\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/cs231n/assignments/project/flowers\n","bash: get_datasets.sh: No such file or directory\n","/content/drive/My Drive/cs231n/assignments/project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z19bVm7o9zeu","trusted":true,"_uuid":"4902ee7b7f4d66a42d59b971180bba213d0133c9","executionInfo":{"status":"ok","timestamp":1622586579831,"user_tz":420,"elapsed":816,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}}},"source":["# Ignore  the warnings\n","import warnings\n","warnings.filterwarnings('always')\n","warnings.filterwarnings('ignore')\n","\n","# data visualisation and manipulation\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib import style\n","import seaborn as sns\n"," \n","#configure\n","# sets matplotlib to inline and displays graphs below the corressponding cell.\n","%matplotlib inline  \n","style.use('fivethirtyeight')\n","sns.set(style='whitegrid',color_codes=True)\n","\n","#model selection\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import LabelEncoder\n","\n","#preprocess.\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","#dl libraraies\n","from keras import backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n","\n","\n","from tensorflow.keras.utils import to_categorical\n","from keras.callbacks import ReduceLROnPlateau\n","\n","# specifically for cnn\n","from keras.layers import Dropout, Flatten,Activation\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n"," \n","import tensorflow as tf\n","import random as rn\n","\n","# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\n","import cv2                  \n","import numpy as np  \n","from tqdm import tqdm\n","import os                   \n","from random import shuffle  \n","from zipfile import ZipFile\n","from PIL import Image\n","\n","#TL pecific modules\n","from keras.applications.vgg16 import VGG16"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"346f2fdc1b809eb19158d411af4288d83c79f389","colab":{"base_uri":"https://localhost:8080/"},"id":"0TvTNjLftcgt","executionInfo":{"status":"ok","timestamp":1622586583796,"user_tz":420,"elapsed":142,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}},"outputId":"e10e80a6-eeaa-4186-ef2e-f4b9400fd802"},"source":["import os\n","path = '/content/drive/My Drive/{}'.format(FOLDERNAME)\n","print(os.listdir(f'{path}/flowers'))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["['flowers', 'rose', 'dandelion', 'daisy', 'sunflower', 'tulip']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"_uuid":"1fa0f050be4d3796a3253bc22c0d87393d55902a","id":"mqcAeAl9tcgt"},"source":["<a id=\"content2\"></a>\n","## 2 ) Preparing the Data"]},{"cell_type":"markdown","metadata":{"_uuid":"192e68db5329da0e491aba8f12a6ca5cda534ce9","id":"WzDuvJT-tcgt"},"source":["## 2.1) Making the functions to get the training and validation set from the Images"]},{"cell_type":"code","metadata":{"id":"abZS8dPk9ze1","trusted":true,"_uuid":"7b0c13e69deaf6449739ba2104bb6238be376f05","executionInfo":{"status":"ok","timestamp":1622586596499,"user_tz":420,"elapsed":104,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}}},"source":["X=[]\n","Z=[]\n","IMG_SIZE=150\n","FLOWER_DAISY_DIR= f'{path}/flowers/daisy'\n","FLOWER_SUNFLOWER_DIR=f'{path}/flowers/sunflower'\n","FLOWER_TULIP_DIR=f'{path}/flowers/tulip'\n","FLOWER_DANDI_DIR=f'{path}/flowers/dandelion'\n","FLOWER_ROSE_DIR=f'{path}/flowers/rose'\n","weights_path='../input/trans-learn-weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qPgwo1d9ze4","trusted":true,"_uuid":"1c467392d43ee29671c5498bd1feea5db5ef862d","executionInfo":{"status":"ok","timestamp":1622586599890,"user_tz":420,"elapsed":117,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}}},"source":["def assign_label(img,flower_type):\n","    return flower_type\n","    "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"vlY8PywM9ze7","trusted":true,"_uuid":"861b4e251d97f7601a3bc2c3077183de4122e3d9","executionInfo":{"status":"ok","timestamp":1622586608535,"user_tz":420,"elapsed":162,"user":{"displayName":"Mehrnaz B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaCV5m2H_od77fWW_j8T9E5_Nd_t82pjGdydJF=s64","userId":"14896880010903859463"}}},"source":["def make_train_data(flower_type,DIR):\n","    for img in tqdm(os.listdir(DIR)):\n","        label=assign_label(img,flower_type)\n","        path = os.path.join(DIR,img)\n","        img = cv2.imread(path,cv2.IMREAD_COLOR)\n","        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n","        \n","        X.append(np.array(img))\n","        Z.append(str(label))\n","        \n","        \n","        "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9hn_RjL29ze_","trusted":true,"_uuid":"04ee1723256e1836f56600bf041e67d1d8370314","outputId":"a158fa49-13f2-4e6f-9064-4f7ba1280fb0"},"source":["make_train_data('Daisy',FLOWER_DAISY_DIR)\n","print(len(X))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 30%|██▉       | 230/769 [01:01<02:34,  3.48it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"6XZpRkLK9zfC","trusted":true,"_uuid":"d4cd8f4d88756086d747d72cad465433c7e1e0ff"},"source":["make_train_data('Sunflower',FLOWER_SUNFLOWER_DIR)\n","print(len(X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvJMmnNx9zfH","trusted":true,"_uuid":"9c86247c9d3651de45e3f6883df1453cf594b65b"},"source":["make_train_data('Tulip',FLOWER_TULIP_DIR)\n","print(len(X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgYj8x-H9zfL","trusted":true,"_uuid":"0ff3f7e5d2aae2b06fe87a49cf89007833d5c83d"},"source":["make_train_data('Dandelion',FLOWER_DANDI_DIR)\n","print(len(X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mtQH6Vg9zfQ","trusted":true,"_uuid":"8e6a148321afbfaa53dfc05a5d700c5a24fed336"},"source":["make_train_data('Rose',FLOWER_ROSE_DIR)\n","print(len(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"997bc5f5a06532e466cabe21ff7cf3fa482f7eb9","id":"a-ojKr7ktcgv"},"source":["## 2.2 ) Visualizing some Random Images"]},{"cell_type":"code","metadata":{"id":"gPolpADLYv9p","trusted":true,"_uuid":"8ad3563c162ea79510b967f9c2a53c0cd6fbc2d4"},"source":["fig,ax=plt.subplots(5,2)\n","fig.set_size_inches(15,15)\n","for i in range(5):\n","    for j in range (2):\n","        l=rn.randint(0,len(Z))\n","        ax[i,j].imshow(X[l])\n","        ax[i,j].set_title('Flower: '+Z[l])\n","        \n","plt.tight_layout()\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"963b723dd10cb918fa045588a5dfa07d55007db3","id":"-z8M8P6Atcgw"},"source":["## 2.3 ) Label Encoding the Y array (i.e. Daisy->0, Rose->1 etc...) & then One Hot Encoding "]},{"cell_type":"code","metadata":{"id":"_Gug0CHU9zfe","trusted":true,"_uuid":"1f006ba66f46d8c3355ecdd3b27b37bdea635944"},"source":["le=LabelEncoder()\n","Y=le.fit_transform(Z)\n","Y=to_categorical(Y,5)\n","X=np.array(X)\n","X=X/255"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d6a8f93a1446fa61e9c89a61be3a0fbb4328ad15","id":"bSLA4CDhtcgx"},"source":["## 2.4 ) Splitting into Training and Validation Sets"]},{"cell_type":"code","metadata":{"id":"4xogXfvm9zfg","trusted":true,"_uuid":"e9b04ed0a732e99941d6a347a14d66eb2cb4727b"},"source":["x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"10556b8fcdde09321078b4fa4df9e9dc651f877a","id":"ZcoHx9U3tcgy"},"source":["## 2.5 ) Setting the Random Seeds"]},{"cell_type":"code","metadata":{"id":"S_nM3vLf9zfj","trusted":true,"_uuid":"f0e2ae22e3bca8e3143d5b4d312460283b833878"},"source":["np.random.seed(42)\n","rn.seed(42)\n","tf.set_random_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"82aa7c88a3fac8f8596ce70653900664f2b61ad4","id":"BrI2BJJ-tcgy"},"source":["<a id=\"content3\"></a>\n","## 3 ) Modelling"]},{"cell_type":"markdown","metadata":{"_uuid":"550936cbbc153229adb624c2c31050c5d93c2a8e","id":"0kSe1otVtcgy"},"source":["## 3.1 ) Specifying the Base Model"]},{"cell_type":"markdown","metadata":{"_uuid":"42730871d9f28580dab21b9e15c6b65c67e8a5e6","id":"RorGj3Y0tcgz"},"source":["Transfer learning refers to using a pretrained model on some other task for your own task. Hence we need to specify the particular model which we are deploying in our task and thus needs to specify the base model.\n","\n","In our case we are using the VGG16 model from the Keras.Applications library as the base model."]},{"cell_type":"code","metadata":{"id":"SERVVhIgkxXV","trusted":true,"_uuid":"33b31b3df570b37c968f4c4e81ea24b48c864caa"},"source":["base_model=VGG16(include_top=False, weights=None,input_shape=(150,150,3), pooling='avg')\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"9155aceff9ad2d42cbd5b9151389d2f15e8c7abd","id":"iap7xkPgtcgz"},"source":["base_model.load_weights(weights_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"0113e4773b50a6aa8434865fe1657d6859d25d71","id":"ZMKj3rMEtcgz"},"source":["#### BREAKING IT DOWN--\n","\n","1) Firstly we import the VGG16 module from the Keras library.\n","\n","2) Next we need to specify if we want  to use the fully connected layers of the VGG16 module or own layers. Since our task is different and we have only 5 target classes we need to have our own layers and I have specified the 'include_top' arguement as 'False'.\n","\n","3) Next we need to specify the weights to be used by the model. Since I want it to use the weights it was trained on in ImageNet competition, I have loaded the weights from the corressponding file. You can directly specify the weights arguement as 'imagenet' in VGG16( )  but it didn't work in my case so I have to explicitily load the weghts from a file.\n"," \n","4) Lastly we just need to specify the shape of the imput that our model need to expect and also specify the 'pooling' type."]},{"cell_type":"code","metadata":{"id":"xHOoktp-k4tw","trusted":true,"_uuid":"3bca9ce0ba74b4bec90b228d7805ce6d64e00062"},"source":["base_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"33a8724542cdac2b9002fbbd3db0400ebedcac0d","id":"9oBrWeA5tcg0"},"source":["Note that this is NOT the summary of our model and this is the summary or the ARCHITECTURE of the VGG16 model that we are deploying as the base model."]},{"cell_type":"markdown","metadata":{"_uuid":"fe17a74b47da287a0b8763096d1e47beac1be85f","id":"5uAII6e8tcg0"},"source":["## 3.2 ) Adding our Own Fully Connected Layers"]},{"cell_type":"markdown","metadata":{"_uuid":"c1285074e54048895fa46f28ce94327dcb1fd2a7","id":"51hiGxYjtcg0"},"source":["Now we need to add at the top of the base model some fully connected layers. Alsowe can use the BatchNormalization and the Dropout layers as usual in case we want to.\n","\n","For this I have used a Keras sequential model and build our entire model on top of it; comprising of the VGG model as the base model + our own fully connected layers."]},{"cell_type":"code","metadata":{"id":"gGgMscM_eIYS","trusted":true,"_uuid":"61461000564043bcdd7dcafc2f99bf931eaecfa0"},"source":["model=Sequential()\n","model.add(base_model)\n","\n","model.add(Dense(256,activation='relu'))\n","model.add(Dense(5,activation='softmax'))\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3d14eeca7ad5f8e01a4d868a2dda5dc4111387fb","id":"gZzF60Mmtcg0"},"source":["## 3.3 ) Data Augmentation to prevent Overfitting"]},{"cell_type":"code","metadata":{"id":"lH038cfsgkvZ","trusted":true,"_uuid":"5859d726c293f373e7ee955bb931bd28f39db8d5"},"source":["datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n","        zoom_range = 0.1, # Randomly zoom image \n","        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n","        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n","        horizontal_flip=True,  # randomly flip images\n","        vertical_flip=False)  # randomly flip images\n","\n","\n","datagen.fit(x_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"fe28d78ca7d6f7af58b8bbaa6c4cf6a377f725b5","id":"lMP5q21btcg1"},"source":["## 3.4 ) Using a Learning Rate Annealer & the Summary"]},{"cell_type":"code","metadata":{"id":"B_6-fsX6gky4","trusted":true,"_uuid":"0953cdcd6425381a3551aeb28af6c4b61654849d"},"source":["epochs=50\n","batch_size=128\n","red_lr=ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=0.0001, patience=2, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IVxD9F-TeIdi","trusted":true,"_uuid":"c18c97375cbdde67228b22780bff5a0230598c71"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"9e930633508ea7464b06a0e6081f3d162537de46","id":"5nrwVxI6tcg1"},"source":["This is now the complete summary of our model that we shall use to classify the images."]},{"cell_type":"markdown","metadata":{"_uuid":"5a36c2ecfbc0b8dc2407be3e685ce235e420a198","id":"Z0YD3lfBtcg1"},"source":["## 3.5 ) Compiling & Training the Model"]},{"cell_type":"markdown","metadata":{"_uuid":"73afda7629edabaed5ed8d2853c78b4f6f08aa61","id":"XcJ8lX48tcg1"},"source":["#### 3.5.1 ) USING BASE MODEL AS A FEATURE EXTRACTOR."]},{"cell_type":"markdown","metadata":{"_uuid":"3a91c19dfbb62a2a063e1a946b7cf0489782c9be","id":"foKdMLdatcg1"},"source":["While using transfer learning in ConvNet; we have basically have 3 main approaches-->\n","\n","1) To use the pretrained model as a feature extractor and just train your classifier on top of it. In this method we do not tune any weights of the model.\n","\n","2) Fine Tuning- In this approach we tune the weights of the pretrained model. This can be done by unfreezing the layers that we want to train.In that case these layers will be initialised with their trained weights on imagenet.\n","\n","3) Lasty we can use a pretrained model.\n","\n","Note that in this section I have used the first approach ie I have just use the conv layers and added my own fully connected layers on top of VGG model. Thus I have trained a classifier on top of the CNN codes."]},{"cell_type":"code","metadata":{"id":"CqU7SQXzTNof","trusted":true,"_uuid":"d2bc03ec5faf3e57a7504680d065a8ba30e403e3"},"source":["base_model.trainable=False # setting the VGG model to be untrainable."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hx2arlHIeIhS","trusted":true,"_uuid":"b5fc67aea4c22dec0cf54adc0d475a9aa49e3dc7"},"source":["model.compile(optimizer=Adam(lr=1e-4),loss='categorical_crossentropy',metrics=['accuracy'])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cS9WpEjOeIjz","trusted":true,"_uuid":"2dc473930156c22f817d91349cd3aa233e623d65"},"source":["History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n","                              epochs = 50, validation_data = (x_test,y_test),\n","                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"7957e2a9e323de85d99f72cf1af44f078ccc5e11","id":"8MRNUXM1tcg2"},"source":["#### 3.5.2 ) FINE TUNING BY UNFREEZING THE LAST BLOCK OF VGG16"]},{"cell_type":"markdown","metadata":{"_uuid":"0deafdf0a87be6b94cf8e34e4e69e0c525afd1b0","id":"ILVy9eWMtcg3"},"source":["In this section I have done fine tuning. To see the effect of the fine tuning I have first unfreezed the last block of the VGG16 model and have set it to trainable."]},{"cell_type":"code","metadata":{"id":"ZcwBRzYYPVx8","trusted":true,"_uuid":"1efcc1bd4b8c809998052b4edf9bfd72a7467ad9"},"source":["for i in range (len(base_model.layers)):\n","    print (i,base_model.layers[i])\n","  \n","for layer in base_model.layers[15:]:\n","    layer.trainable=True\n","for layer in base_model.layers[0:15]:\n","    layer.trainable=False\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6-PRjikXAoK","trusted":true,"_uuid":"67f268c689a28c44745f42658b1937d54ea2504c"},"source":["model.compile(optimizer=Adam(lr=1e-4),loss='categorical_crossentropy',metrics=['accuracy'])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8HOQuHdyTfj9","trusted":true,"_uuid":"21fbf26cc0c2aab15e7c5903c2f92ef6a736c213"},"source":["History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n","                              epochs = 50, validation_data = (x_test,y_test),\n","                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"9545f9161aa1643df2c00753121bb87b7a7c87d6","id":"bxBqvrvbtcg3"},"source":["#### Note that the validation accuracy on fine tuning by unfreezing the last block of the VGG16 model has increased to about 81% ; almost by 3% as compared to the case when we run a classifier on the top of the CNN codes in previous section."]},{"cell_type":"code","metadata":{"id":"fhEnWLenslUN","trusted":true,"_uuid":"131a4ba19320a3fde1e5a485cd1d77e2af48ae88"},"source":["plt.plot(History.history['acc'])\n","plt.plot(History.history['val_acc'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","plt.legend(['train', 'test'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYuLuLAQslnF","trusted":true,"_uuid":"954611cab64452acf57fb7dd4c08c4ffc6e1812f"},"source":["plt.plot(History.history['loss'])\n","plt.plot(History.history['val_loss'])\n","plt.title('Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.legend(['train', 'test'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"4f05de21b7a21287091274b00365d470f3acae3b","id":"LDeSbSg7tcg4"},"source":["#### 3.5.3) UNFREEZING THE LAST 2 BLOCKS"]},{"cell_type":"markdown","metadata":{"_uuid":"1073c9b152e75cd68a88b4b224a409c0c7c60f0c","id":"UASTp8sztcg4"},"source":["Similarly unffreezing the last 2 blocks of the VGG16model."]},{"cell_type":"code","metadata":{"id":"mPW6EdQ6zsVG","trusted":true,"_uuid":"9244d358ff5774c52799bd2009a9b27cce6ce00c"},"source":["for i in range (len(base_model.layers)):\n","    print (i,base_model.layers[i])\n","  \n","for layer in base_model.layers[11:]:\n","    layer.trainable=True\n","for layer in base_model.layers[0:11]:\n","    layer.trainable=False\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8r2vkhzxzri4","trusted":true,"_uuid":"82a58b8e1b1a8bf539377e53d3e9129f9dfa838c"},"source":["model.compile(optimizer=Adam(lr=1e-4),loss='categorical_crossentropy',metrics=['accuracy'])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mu4-v7Tl0JnF","trusted":true,"_uuid":"af35be1c41a5b83459a9ea7f1b544ec62fcee54f"},"source":["History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n","                              epochs = 50, validation_data = (x_test,y_test),\n","                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"2e2aed33b9f03da420eea3eaea02f2d19f8c78b7","id":"idpbC58qtcg4"},"source":["#### Note that there is still an increse in validation accuracy of about 1.5% and the same has now reached to about 81.5%."]},{"cell_type":"code","metadata":{"id":"uVG9lwNqahjL","trusted":true,"_uuid":"e662853bfaf2f12c0bba1a311a5498d3eea618bc"},"source":["plt.plot(History.history['loss'])\n","plt.plot(History.history['val_loss'])\n","plt.title('Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.legend(['train', 'test'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_R7EV33ahsp","trusted":true,"_uuid":"3a6beed19c34da7cbd224bc733b23bc81bc232ce"},"source":["plt.plot(History.history['acc'])\n","plt.plot(History.history['val_acc'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","plt.legend(['train', 'test'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"4792c7b10230461e4bac5080185188bc29edddc2","id":"AWAAT9iwtcg5"},"source":["#### 3.5.4 ) TRAINING THE ENTIRE MODEL FROM SCRATCH"]},{"cell_type":"markdown","metadata":{"_uuid":"77ba60ecd1669bc44a948a71c282b15fe7443829","id":"Cbi7yqCQtcg5"},"source":["Finally I have tried to train the model from scratch. Note this is not reasonable though as our data is also not much similar with the imagenet data plus we are quite short of data as we only have around 4200 images.\n","\n","Hence this model is quite prone to overfitting and I have done this just to check that the results validate with the though-process."]},{"cell_type":"code","metadata":{"id":"NoEsKBey0Yiw","trusted":true,"_uuid":"764f67047b47d9f9c2fea3fad5630cc2fd60f377"},"source":["model=Sequential()\n","model.add(base_model)\n","\n","model.add(Dense(256,activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(5,activation='softmax'))\n","\n","\n","for layer in base_model.layers:\n","    layer.trainable=True\n","\n","model.compile(optimizer=Adam(lr=1e-4),loss='categorical_crossentropy',metrics=['accuracy'])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFbqDROC0Zqv","trusted":true,"_uuid":"adba788f8da8644c9e268bfe4063ecf8623e5e3b"},"source":["History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n","                              epochs = 50, validation_data = (x_test,y_test),\n","                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w44nLg9rZ7oY","trusted":false,"_uuid":"9f55b5b6a11cb49117b92960ff7925a8a03b836e"},"source":["plt.plot(History.history['acc'])\n","plt.plot(History.history['val_acc'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","plt.legend(['train', 'test'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"f7abdc1be979a2e325c6dc2b2c4b5ef2b786d350","id":"qR_NfDWmtcg6"},"source":["#### Note that the graphs as well as the results clearly show that there is significant overfitting. Also note that despite the overfitting the overall validartion accuracy has though increased from previous best of about 0.815 to a whopping 0.93."]},{"cell_type":"markdown","metadata":{"_uuid":"f5e070bcec135a62ace22eecce508f6599459f3c","id":"fqNF1Vxgtcg6"},"source":["<a id=\"content4\"></a>\n","## 4 ) Visualizing Predictons on the Validation Set"]},{"cell_type":"code","metadata":{"id":"7ozBQTvNeIxE","trusted":false,"_uuid":"5e432418128f23165b56338b57be998da2e1d3a7"},"source":["# getting predictions on val set.\n","pred=model.predict(x_test)\n","pred_digits=np.argmax(pred,axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JouGgHHeI0f","trusted":false,"_uuid":"c3e12af455fa0ef0ff5466c186a83e0a5489a2e9"},"source":["# now storing some properly as well as misclassified indexes'.\n","i=0\n","prop_class=[]\n","mis_class=[]\n","\n","for i in range(len(y_test)):\n","    if(np.argmax(y_test[i])==pred_digits[i]):\n","        prop_class.append(i)\n","    if(len(prop_class)==8):\n","        break\n","\n","i=0\n","for i in range(len(y_test)):\n","    if(not np.argmax(y_test[i])==pred_digits[i]):\n","        mis_class.append(i)\n","    if(len(mis_class)==8):\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"5024d50471de5995166542ffa9580e7609431527","id":"C184vToRtcg6"},"source":["#### CORRECTLY CLASSIFIED FLOWER IMAGES"]},{"cell_type":"code","metadata":{"id":"HMJGc9SH8VFP","trusted":false,"_uuid":"61a5c64c0b8d5581d3acf6b35e7092ad36bb1d54"},"source":["warnings.filterwarnings('ignore')\n","\n","count=0\n","fig,ax=plt.subplots(4,2)\n","fig.set_size_inches(15,15)\n","for i in range (4):\n","    for j in range (2):\n","        ax[i,j].imshow(x_test[prop_class[count]])\n","        ax[i,j].set_title(\"Predicted Flower : \"+str(le.inverse_transform([pred_digits[prop_class[count]]]))+\"\\n\"+\"Actual Flower : \"+str(le.inverse_transform(np.argmax([y_test[prop_class[count]]]))))\n","        plt.tight_layout()\n","        count+=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"0775db9256a74c432b30c3410c8b6263aa9cc15f","id":"-OFjQ3Xetcg7"},"source":["#### MISCLASSIFIED IMAGES OF FLOWERS"]},{"cell_type":"code","metadata":{"id":"BpAAENzv8X7F","trusted":false,"_uuid":"3b1783ea4d983c0426b18ea1d740d5f44ca43650"},"source":["warnings.filterwarnings('always')\n","warnings.filterwarnings('ignore')\n","\n","count=0\n","fig,ax=plt.subplots(4,2)\n","fig.set_size_inches(15,15)\n","for i in range (4):\n","    for j in range (2):\n","        ax[i,j].imshow(x_test[mis_class[count]])\n","        ax[i,j].set_title(\"Predicted Flower : \"+str(le.inverse_transform([pred_digits[mis_class[count]]]))+\"\\n\"+\"Actual Flower : \"+str(le.inverse_transform(np.argmax([y_test[mis_class[count]]]))))\n","        plt.tight_layout()\n","        count+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5KnJJlkMNrpO","trusted":false,"_uuid":"2845b3a1a171aa0d9398444037f355a22ea25b9e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWwASowbba12","_uuid":"927a9385f09a0a24d15bb7c77155462a5fabaa8c"},"source":["## THE END."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"9e3f20b1bcdba6df317b4327c330a92535185459","id":"LuPq8rdOtcg7"},"source":["## [ Please star/upvote if u like it. ]"]},{"cell_type":"code","metadata":{"trusted":false,"_uuid":"52eb4c22f88ffa062c479d1114ebb9c856eb14b6","id":"HXtrIOjvtcg7"},"source":[""],"execution_count":null,"outputs":[]}]}